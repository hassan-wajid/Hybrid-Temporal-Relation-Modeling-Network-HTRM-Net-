{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:57:02.332421Z",
     "iopub.status.busy": "2025-02-18T14:57:02.332064Z",
     "iopub.status.idle": "2025-02-18T14:57:04.028128Z",
     "shell.execute_reply": "2025-02-18T14:57:04.027461Z",
     "shell.execute_reply.started": "2025-02-18T14:57:02.332396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeatureEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Block 1: Feature Encoding \n",
    "    -------------------------\n",
    "    Applies a simple 2D convolution per frame, followed by\n",
    "    an adaptive max pooling to produce (B, T, out_channels).\n",
    "    NOTE: In the original paper, the Video Swin Transformer was used; \n",
    "    instead, I used a simple Video Encoder due to hardware limitations.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=512):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=3, \n",
    "            padding=1\n",
    "        )\n",
    "        self.pool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (B, T, C, H, W)\n",
    "          B: Batch size\n",
    "          T: Number of frames\n",
    "          C: Number of channels (3 for RGB)\n",
    "          H,W: Frame height and width\n",
    "        Returns:\n",
    "          (B, T, out_channels)\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "        # Merge B and T to process each frame individually\n",
    "        x = x.view(B * T, C, H, W)  # shape: (B*T, C, H, W)\n",
    "        \n",
    "        # Convolution\n",
    "        x = F.relu(self.conv(x))    # shape: (B*T, out_channels, H, W)\n",
    "        \n",
    "        # Pooling\n",
    "        x = self.pool(x)           # shape: (B*T, out_channels, 1, 1)\n",
    "        \n",
    "        # Restore (B, T) as separate dimensions\n",
    "        x = x.view(B, T, -1)       # shape: (B, T, out_channels)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:57:06.159545Z",
     "iopub.status.busy": "2025-02-18T14:57:06.159059Z",
     "iopub.status.idle": "2025-02-18T14:57:06.168693Z",
     "shell.execute_reply": "2025-02-18T14:57:06.167709Z",
     "shell.execute_reply.started": "2025-02-18T14:57:06.159512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TemporalRelationModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Block 2: Hybrid Temporal Relation Modeling\n",
    "    ------------------------------------------\n",
    "    Generates a bi-modal self-similarity matrix using multi-head\n",
    "    self-attention & dual-softmax, applies dropout for random matrix\n",
    "    dropping, and uses 1D convolution for local context.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=512, num_heads=4, dropout_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.feature_dim = feature_dim\n",
    "        self.head_dim = feature_dim // num_heads\n",
    "        \n",
    "        # Linear layers to get queries and keys\n",
    "        self.query_linear = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_linear = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        # Dropout to simulate random matrix dropping\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Local Temporal Context Modeling (1D separable convolution)\n",
    "        self.depthwise_conv = nn.Conv1d(\n",
    "            in_channels=feature_dim, \n",
    "            out_channels=feature_dim, \n",
    "            kernel_size=5, \n",
    "            padding=2,\n",
    "            groups=feature_dim  # depthwise\n",
    "        )\n",
    "        self.pointwise_conv = nn.Conv1d(\n",
    "            in_channels=feature_dim, \n",
    "            out_channels=feature_dim, \n",
    "            kernel_size=1\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (B, T, feature_dim)\n",
    "          B: batch size\n",
    "          T: number of frames\n",
    "          feature_dim: dimension of per-frame feature\n",
    "        Returns:\n",
    "          (B, 2, T, T)  # 2 channels: [global_sim, local_sim]\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        \n",
    "        # ---- (1) Bi-modal Self-Attention Similarity ----\n",
    "        Q = self.query_linear(x)  # (B, T, D)\n",
    "        K = self.key_linear(x)    # (B, T, D)\n",
    "        \n",
    "        # Reshape into multiple heads: (B, num_heads, T, head_dim)\n",
    "        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Self-attention-based similarity\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim**0.5)\n",
    "        attn_matrix1 = torch.softmax(attn_scores, dim=-1)  # row-wise softmax\n",
    "        \n",
    "        # Dual-softmax\n",
    "        raw_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        softmax_row = torch.softmax(raw_scores, dim=-1)\n",
    "        softmax_col = torch.softmax(raw_scores, dim=-2)\n",
    "        attn_matrix2 = softmax_row * softmax_col\n",
    "        \n",
    "        # Combine and drop\n",
    "        attn_matrix = attn_matrix1 + attn_matrix2\n",
    "        attn_matrix = self.dropout(attn_matrix)  # (B, num_heads, T, T)\n",
    "        \n",
    "        # Average across heads\n",
    "        attn_matrix_avg = attn_matrix.mean(dim=1, keepdim=True)  # (B, 1, T, T)\n",
    "        \n",
    "        # ---- (2) Local Temporal Context Modeling ----\n",
    "        # Depthwise + pointwise conv on the temporal dimension\n",
    "        local_context = x.transpose(1, 2)  # (B, D, T)\n",
    "        local_context = self.depthwise_conv(local_context)\n",
    "        local_context = self.pointwise_conv(local_context)\n",
    "        local_context = local_context.transpose(1, 2)  # (B, T, D)\n",
    "        \n",
    "        # Build local similarity\n",
    "        local_sim = torch.matmul(local_context, local_context.transpose(-2, -1))  # (B, T, T)\n",
    "        local_sim = local_sim.unsqueeze(1)  # (B, 1, T, T)\n",
    "        \n",
    "        # ---- (3) Concatenate global and local similarity\n",
    "        combined = torch.cat([attn_matrix_avg, local_sim], dim=1)  # (B, 2, T, T)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:57:06.466706Z",
     "iopub.status.busy": "2025-02-18T14:57:06.466430Z",
     "iopub.status.idle": "2025-02-18T14:57:06.472098Z",
     "shell.execute_reply": "2025-02-18T14:57:06.471210Z",
     "shell.execute_reply.started": "2025-02-18T14:57:06.466686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiScaleFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Block 3: Multi-scale Self-Similarity Fusion\n",
    "    ------------------------------------------\n",
    "    Fuses a list of self-similarity matrices (from multiple scales)\n",
    "    by stacking them and applying a 3D convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_channels: total channels after stacking all scales\n",
    "          out_channels: number of output channels after fusion\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv3d = nn.Conv3d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_list):\n",
    "        \"\"\"\n",
    "        x_list: list of tensors, each shape (B, C, T, T).\n",
    "                Suppose we have N scales, each with C channels.\n",
    "        Returns:\n",
    "          fused: shape (B, out_channels, T, T)\n",
    "        \"\"\"\n",
    "        # 1) Stack along a new dimension => shape (B, N, C, T, T)\n",
    "        x = torch.stack(x_list, dim=1)  \n",
    "        \n",
    "        B, N, C, T, _ = x.shape  # N = number of scales\n",
    "        \n",
    "        # 2) Merge the scale dim (N) and channel dim (C) => (B, N*C, T, T)\n",
    "        x = x.view(B, N * C, T, T)\n",
    "        \n",
    "        # 3) Add a dummy 'depth' dimension => (B, N*C, 1, T, T)\n",
    "        x = x.unsqueeze(2)\n",
    "        \n",
    "        # 4) Apply 3D convolution => (B, out_channels, 1, T, T)\n",
    "        fused = self.conv3d(x)\n",
    "        \n",
    "        # 5) Squeeze out the dummy dimension => (B, out_channels, T, T)\n",
    "        fused = fused.squeeze(2)\n",
    "        return fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:57:10.805340Z",
     "iopub.status.busy": "2025-02-18T14:57:10.805023Z",
     "iopub.status.idle": "2025-02-18T14:57:10.811955Z",
     "shell.execute_reply": "2025-02-18T14:57:10.811066Z",
     "shell.execute_reply.started": "2025-02-18T14:57:10.805314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DensityMapRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Block 4: Density Map Regression\n",
    "    -------------------------------\n",
    "    Uses a transformer encoder (1 layer) followed by\n",
    "    an MLP to produce a 1D density map over frames.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_dim, output_dim=1, num_frames=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_features: Number of channels in the fused feature (from block 3).\n",
    "          hidden_dim: Hidden dimension in the MLP.\n",
    "          output_dim: Size of final output per frame (1 for a density value).\n",
    "          num_frames: Number of frames (T).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.transformer = nn.TransformerEncoderLayer(\n",
    "            d_model=in_features, \n",
    "            nhead=4\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_frames * in_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_frames * output_dim)\n",
    "        )\n",
    "        self.num_frames = num_frames\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (B, C, T, T)  # from block 3\n",
    "          B: Batch size\n",
    "          C: # of channels (in_features)\n",
    "          T: # of frames\n",
    "        \n",
    "        Returns:\n",
    "          A density map of shape (B, T).\n",
    "        \"\"\"\n",
    "        B, C, T, _ = x.shape\n",
    "        \n",
    "        # 1) Average across one T dimension => (B, C, T)\n",
    "        #    This is a simple approach to get \"per-frame\" features from a T x T matrix\n",
    "        x_avg = x.mean(dim=-1)   # shape: (B, C, T)\n",
    "        \n",
    "        # 2) Switch to (B, T, C) for the transformer\n",
    "        x_avg = x_avg.transpose(1, 2)  # (B, T, C)\n",
    "        \n",
    "        # 3) Pass through the transformer encoder\n",
    "        x_trans = self.transformer(x_avg)  # (B, T, C)\n",
    "        \n",
    "        # 4) Flatten => (B, T*C)\n",
    "        x_flat = x_trans.reshape(B, -1)\n",
    "        \n",
    "        # 5) MLP => (B, T * output_dim)\n",
    "        density = self.mlp(x_flat)\n",
    "        \n",
    "        # 6) Reshape => (B, T)\n",
    "        density = density.view(B, T)\n",
    "        \n",
    "        return density\n",
    "\n",
    "def temporal_downsample(x, factor):\n",
    "    return x[:, ::factor, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:57:12.411272Z",
     "iopub.status.busy": "2025-02-18T14:57:12.410996Z",
     "iopub.status.idle": "2025-02-18T14:57:12.417878Z",
     "shell.execute_reply": "2025-02-18T14:57:12.416970Z",
     "shell.execute_reply.started": "2025-02-18T14:57:12.411251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HTRMNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, feature_dim=512, hidden_dim=128, out_channels_fusion=8, num_frames=64):\n",
    "        super().__init__()\n",
    "        self.num_frames = num_frames\n",
    "        \n",
    "        self.feature_encoder = FeatureEncoder(in_channels, feature_dim)\n",
    "        self.relation_module = TemporalRelationModule(feature_dim, num_heads=4, dropout_prob=0.3)\n",
    "        self.fusion = MultiScaleFusion(in_channels=6, out_channels=out_channels_fusion)\n",
    "        self.regressor = DensityMapRegressor(in_features=out_channels_fusion, hidden_dim=hidden_dim, num_frames=num_frames)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_encoder(x)  # (B, T, D)\n",
    "        \n",
    "        # multi-scale\n",
    "        s1 = features\n",
    "        s2 = temporal_downsample(features, 2)\n",
    "        s3 = temporal_downsample(features, 4)\n",
    "        \n",
    "        sim1 = self.relation_module(s1)  # (B, 2, T, T)\n",
    "        sim2 = self.relation_module(s2)  # (B, 2, T/2, T/2)\n",
    "        sim3 = self.relation_module(s3)  # (B, 2, T/4, T/4)\n",
    "        \n",
    "        # upsample sim2, sim3\n",
    "        sim2_up = nn.functional.interpolate(sim2, size=(self.num_frames, self.num_frames), mode='bilinear', align_corners=False)\n",
    "        sim3_up = nn.functional.interpolate(sim3, size=(self.num_frames, self.num_frames), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        fused = self.fusion([sim1, sim2_up, sim3_up])  # (B, out_channels_fusion, T, T)\n",
    "        density_map = self.regressor(fused)            # (B, T)\n",
    "        return density_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:57:19.984990Z",
     "iopub.status.busy": "2025-02-18T14:57:19.984707Z",
     "iopub.status.idle": "2025-02-18T14:57:45.945219Z",
     "shell.execute_reply": "2025-02-18T14:57:45.944499Z",
     "shell.execute_reply.started": "2025-02-18T14:57:19.984968Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/15 ===\n",
      "Step [1/5], Loss: 0.4747\n",
      "Step [2/5], Loss: 0.4206\n",
      "Step [3/5], Loss: 0.4128\n",
      "Step [4/5], Loss: 0.4319\n",
      "Step [5/5], Loss: 0.4089\n",
      "\n",
      "=== Epoch 2/15 ===\n",
      "Step [1/5], Loss: 0.4570\n",
      "Step [2/5], Loss: 0.4031\n",
      "Step [3/5], Loss: 0.4212\n",
      "Step [4/5], Loss: 0.4063\n",
      "Step [5/5], Loss: 0.3724\n",
      "\n",
      "=== Epoch 3/15 ===\n",
      "Step [1/5], Loss: 0.3947\n",
      "Step [2/5], Loss: 0.3732\n",
      "Step [3/5], Loss: 0.3743\n",
      "Step [4/5], Loss: 0.3577\n",
      "Step [5/5], Loss: 0.3450\n",
      "\n",
      "=== Epoch 4/15 ===\n",
      "Step [1/5], Loss: 0.3562\n",
      "Step [2/5], Loss: 0.3585\n",
      "Step [3/5], Loss: 0.3605\n",
      "Step [4/5], Loss: 0.3542\n",
      "Step [5/5], Loss: 0.2981\n",
      "\n",
      "=== Epoch 5/15 ===\n",
      "Step [1/5], Loss: 0.3055\n",
      "Step [2/5], Loss: 0.3194\n",
      "Step [3/5], Loss: 0.3050\n",
      "Step [4/5], Loss: 0.3039\n",
      "Step [5/5], Loss: 0.3115\n",
      "\n",
      "=== Epoch 6/15 ===\n",
      "Step [1/5], Loss: 0.3461\n",
      "Step [2/5], Loss: 0.2962\n",
      "Step [3/5], Loss: 0.3052\n",
      "Step [4/5], Loss: 0.3061\n",
      "Step [5/5], Loss: 0.3038\n",
      "\n",
      "=== Epoch 7/15 ===\n",
      "Step [1/5], Loss: 0.2798\n",
      "Step [2/5], Loss: 0.2772\n",
      "Step [3/5], Loss: 0.2782\n",
      "Step [4/5], Loss: 0.2619\n",
      "Step [5/5], Loss: 0.2748\n",
      "\n",
      "=== Epoch 8/15 ===\n",
      "Step [1/5], Loss: 0.2686\n",
      "Step [2/5], Loss: 0.2959\n",
      "Step [3/5], Loss: 0.2939\n",
      "Step [4/5], Loss: 0.3049\n",
      "Step [5/5], Loss: 0.2351\n",
      "\n",
      "=== Epoch 9/15 ===\n",
      "Step [1/5], Loss: 0.2865\n",
      "Step [2/5], Loss: 0.2661\n",
      "Step [3/5], Loss: 0.2610\n",
      "Step [4/5], Loss: 0.2527\n",
      "Step [5/5], Loss: 0.2544\n",
      "\n",
      "=== Epoch 10/15 ===\n",
      "Step [1/5], Loss: 0.2216\n",
      "Step [2/5], Loss: 0.2519\n",
      "Step [3/5], Loss: 0.2324\n",
      "Step [4/5], Loss: 0.2465\n",
      "Step [5/5], Loss: 0.2313\n",
      "\n",
      "=== Epoch 11/15 ===\n",
      "Step [1/5], Loss: 0.2689\n",
      "Step [2/5], Loss: 0.2373\n",
      "Step [3/5], Loss: 0.1742\n",
      "Step [4/5], Loss: 0.2112\n",
      "Step [5/5], Loss: 0.1843\n",
      "\n",
      "=== Epoch 12/15 ===\n",
      "Step [1/5], Loss: 0.2418\n",
      "Step [2/5], Loss: 0.2192\n",
      "Step [3/5], Loss: 0.2208\n",
      "Step [4/5], Loss: 0.1933\n",
      "Step [5/5], Loss: 0.2302\n",
      "\n",
      "=== Epoch 13/15 ===\n",
      "Step [1/5], Loss: 0.1760\n",
      "Step [2/5], Loss: 0.2364\n",
      "Step [3/5], Loss: 0.2412\n",
      "Step [4/5], Loss: 0.1929\n",
      "Step [5/5], Loss: 0.1672\n",
      "\n",
      "=== Epoch 14/15 ===\n",
      "Step [1/5], Loss: 0.1795\n",
      "Step [2/5], Loss: 0.1906\n",
      "Step [3/5], Loss: 0.1827\n",
      "Step [4/5], Loss: 0.2111\n",
      "Step [5/5], Loss: 0.2098\n",
      "\n",
      "=== Epoch 15/15 ===\n",
      "Step [1/5], Loss: 0.1668\n",
      "Step [2/5], Loss: 0.1377\n",
      "Step [3/5], Loss: 0.1751\n",
      "Step [4/5], Loss: 0.1888\n",
      "Step [5/5], Loss: 0.1818\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#######################################\n",
    "#. Simple Training Script on Dummy Data\n",
    "#######################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 2\n",
    "    T = 64\n",
    "    H, W = 128, 256   # smaller than 256 for speed\n",
    "    EPOCHS = 15\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = HTRMNet(\n",
    "        in_channels=3, \n",
    "        feature_dim=128,      # smaller for speed\n",
    "        hidden_dim=64, \n",
    "        out_channels_fusion=4,  # smaller for speed\n",
    "        num_frames=T\n",
    "    ).cuda()  # move to GPU if available\n",
    "    \n",
    "    # Define a simple MSE loss for density maps\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Dummy training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{EPOCHS} ===\")\n",
    "        \n",
    "        # In a real scenario, you'd iterate over a DataLoader\n",
    "        # Here, we just create random data for 5 \"batches\"\n",
    "        for step in range(5):\n",
    "            # 1) Create random video (B, T, C, H, W) and random target density (B, T)\n",
    "            dummy_video = torch.randn(BATCH_SIZE, T, 3, H, W).cuda()\n",
    "            # random target density map in [0,1]\n",
    "            dummy_target = torch.rand(BATCH_SIZE, T).cuda()\n",
    "            \n",
    "            # 2) Forward pass\n",
    "            pred_density = model(dummy_video)\n",
    "            \n",
    "            # 3) Compute loss\n",
    "            loss = criterion(pred_density, dummy_target)\n",
    "            \n",
    "            # 4) Backprop and update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f\"Step [{step+1}/5], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T19:54:41.540286Z",
     "iopub.status.busy": "2025-02-17T19:54:41.539930Z",
     "iopub.status.idle": "2025-02-17T19:54:41.835110Z",
     "shell.execute_reply": "2025-02-17T19:54:41.834272Z",
     "shell.execute_reply.started": "2025-02-17T19:54:41.540260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Define model hyperparams\n",
    "T = 64\n",
    "H, W = 128, 128  # Frame height & width\n",
    "BATCH_SIZE = 1   # We'll visualize only 1 sample\n",
    "\n",
    "# 2) Create a dummy video: shape (B, T, C=3, H, W)\n",
    "video = torch.randn(BATCH_SIZE, T, 3, H, W).cuda()\n",
    "\n",
    "# 3) Import or define HTRMNet (from your code)\n",
    "#    If needed, copy your HTRMNet code here.\n",
    "#    For demonstration, we assume you already have the definition.\n",
    "#    We'll just instantiate it:\n",
    "\n",
    "model = HTRMNet(\n",
    "    in_channels=3, \n",
    "    feature_dim=128,      # smaller for speed\n",
    "    hidden_dim=64, \n",
    "    out_channels_fusion=4,  \n",
    "    num_frames=T\n",
    ").cuda()\n",
    "\n",
    "# 4) Forward pass to get density map (B, T)\n",
    "with torch.no_grad():\n",
    "    density_map = model(video)  # shape: (1, 64)\n",
    "\n",
    "# 5) Convert the first sample's density map to CPU NumPy for plotting\n",
    "density_map_np = density_map[0].cpu().numpy()  # shape: (64,)\n",
    "\n",
    "# 6) Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(density_map_np, marker='o', label='Predicted Density')\n",
    "plt.title(\"Predicted Density over Frames\")\n",
    "plt.xlabel(\"Frame Index\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
